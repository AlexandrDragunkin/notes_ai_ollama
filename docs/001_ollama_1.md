–ü—Ä–∏–≤–µ—Ç! –ü–æ–Ω–∏–º–∞—é, –∫–∞–∫ –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å ¬´–º–µ–¥–ª–µ–Ω–Ω–æ¬ª –∏–ª–∏ ¬´—Å–±–æ–∏—Ç¬ª –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ù–∏–∂–µ ‚Äì ¬´—à–ø–∞—Ä–≥–∞–ª–∫–∞¬ª –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ‚ÄØ‚Äî‚ÄØ—á—Ç–æ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –∏ —Å–¥–µ–ª–∞—Ç—å 
—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∫–æ–¥–∞ –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–º.  
–í—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã –∫ –ª—é–±–æ–π –∑–∞–¥–∞—á–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –≤ VS‚ÄØCode, –Ω–æ —è –≤–∫–ª—é—á–∏–ª –∏—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∞—à–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è **roo‚Äëcode** (–∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥–æ *gpt‚Äëoss:20b*).

| –ß—Ç–æ —É–ª—É—á—à–∏—Ç—å | –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å | –ü–æ—á–µ–º—É |
|---------------|-------------|--------|
| **–ú–æ–¥–µ–ª—å** | –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ **7B** –∏–ª–∏ **13B**. <br>–ï—Å–ª–∏ –≤—Å—ë –∂–µ —Ö–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å 20B, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ **quantization** (4‚Äëbit –∏–ª–∏ 8‚Äëbit). | 20‚ÄØB –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç 80‚ÄØ+‚ÄØ–ì–ë VRAM. 7‚Äë13‚ÄØB ‚Äî        
—É–∂–µ 30‚Äë40‚ÄØ–ì–ë, –∞ —Å 4‚Äëbit –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –¥–∞–∂–µ –Ω–∞ 12‚Äë16‚ÄØ–ì–ë GPU. |
| **–ö–æ–Ω—Ç–µ–∫—Å—Ç** | –°–Ω–∏–∑—å—Ç–µ `num_ctx` –¥–æ **512** (–∏–ª–∏ **1024** —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ GPU –ø–æ–∑–≤–æ–ª—è–µ—Ç). –î–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–ª–æ–∫–∞ –∫–æ–¥–∞ –≤–æ–∫—Ä—É–≥ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏/—Ñ–∞–π–ª–∞. | –ú–µ–Ω—å—à–∏–π
–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–Ω–∏–∂–∞–µ—Ç –ø–∞–º—è—Ç—å –∏ –≤—Ä–µ–º—è —Ä–∞—Å—á—ë—Ç–∞, –∞ —Ç–∞–∫–∂–µ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ¬´–ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è¬ª –ø–∞–º—è—Ç–∏. |
| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏** | `max_new_tokens=256`, `temperature=0.0` (–∏–ª–∏ 0.2), `top_p=0.95`. <br>–î–ª—è ¬´—Ç–æ—á–Ω–æ–≥–æ¬ª —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ —Å—Ç–∞–≤—å—Ç–µ `temperature=0`. | –ú–µ–Ω—å—à–∏–µ —Ç–æ–∫–µ–Ω—ã + —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è      
—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É–º–µ–Ω—å—à–∞—é—Ç –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –∏ –¥–µ–ª–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–º. |
| **–ü–∞–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** | –ï—Å–ª–∏ –≤–∞—à —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å batch‚Äëinference, –∑–∞–ø—É—Å—Ç–∏—Ç–µ **vLLM** (–∏–ª–∏ ONNXRuntime + FlashAttention). <br>–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `--max-batch-size 8` (–∏–ª–∏ 16). | vLLM        
–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞–º—è—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –∞ FlashAttention —É—Å–∫–æ—Ä—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é + –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ. |
| **Quantization** | *–ï—Å–ª–∏ –≤—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GPU —Å 20‚ÄØB:* <br>`pip install bitsandbytes==0.41.0` <br>–ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —Ç–∞–∫: <br>`model = 
AutoModelForCausalLM.from_pretrained("gpt-oss/20b", load_in_4bit=True, device_map="auto")` | 4‚Äëbit —ç–∫–æ–Ω–æ–º–∏—Ç ~4‚Äë5√ó –ø–∞–º—è—Ç—å –∏ —É—Å–∫–æ—Ä—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (–≤ —Å—Ä–µ–¥–Ω–µ–º 2‚Äë4√ó). |
| **–ü–∞–º—è—Ç—å** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ `device_map` —É–∫–∞–∑–∞–Ω–æ `"auto"` (–∏–ª–∏ `"balanced"`, `"balanced_low_0"`). <br>–ï—Å–ª–∏ –Ω–∞ GPU –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏, –≤–∫–ª—é—á–∏—Ç–µ `offload_folder` –¥–ª—è CPU‚Äë—Å–≤–æ–ø–∞. |        
–ü–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤–µ—Å –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É GPU –∏ CPU. |
| **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** | –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **lazy loading**: —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. | –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤—Å–µ–≥–æ 
—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. |
| **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å** | –í–∫–ª—é—á–∏—Ç–µ `TOKENIZERS_PARALLELISM=True` –∏ —É–∫–∞–∂–∏—Ç–µ `max_workers=8`. | –£—Å–∫–æ—Ä—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ñ–∞–π–ª–æ–≤. |
| **–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—à–∏–±–æ–∫** | –ï—Å–ª–∏ –º–æ–¥–µ–ª—å ¬´—Å–±–æ–∏—Ç¬ª, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ: <br>‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–µ—Ä—Å–∏—è `transformers` >= 4.34. <br>‚Ä¢ –£–¥–∞–ª–∏—Ç–µ –∫—ç—à (`rm -rf ~/.cache/huggingface`). <br>‚Ä¢ –ï—Å–ª–∏ GPU –Ω–µ 
—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è, –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ CPU‚Äë–∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å 8‚Äë–±–∏—Ç–Ω—ã–º `bitsandbytes`. | –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å ¬´MemoryError¬ª –∏ ¬´CUDA out of memory¬ª. |

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (Python‚Äë—Å–∫—Ä–∏–ø—Ç)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

MODEL_NAME = "gpt-oss/20b"

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    use_fast=True,
    legacy=False
)

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å 4‚Äë–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (–∏–ª–∏ 8‚Äë–±–∏—Ç, –µ—Å–ª–∏ 4‚Äë–±–∏—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",          # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Å–∞
    torch_dtype=torch.float16,  # —É–º–µ–Ω—å—à–∞–µ—Ç –ø–∞–º—è—Ç—å
    load_in_4bit=True,          # 4‚Äë–±–∏—Ç–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    # offload_folder="/tmp/offload",  # –µ—Å–ª–∏ –Ω—É–∂–µ–Ω CPU‚Äë—Å–≤–æ–ø
)

model.eval()

# 3. –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞: —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏–∏
prompt = """
# –§—É–Ω–∫—Ü–∏—è: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—É–º–º—ã –∫–≤–∞–¥—Ä–∞—Ç–æ–≤
def sum_of_squares(nums):
    total = 0
    for n in nums:
        total += n*n
    return total

# –ó–∞–¥–∞—á–∞: —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∫–æ–¥–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –±–æ–ª–µ–µ pythonic
"""

inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to("cuda")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.0,      # deterministic
        top_p=0.95,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
    )

generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated)
```

### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ VS‚ÄØCode —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏

1. **–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç** (–∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–æ –≤—ã–∑—ã–≤–∞–ª–æ —ç—Ç–æ—Ç –∫–æ–¥ —á–µ—Ä–µ–∑ CLI).
2. –í –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:
   - `model_name = gpt-oss/20b`
   - `num_ctx = 512`
   - `max_new_tokens = 256`
   - `temperature = 0.0`
3. –í–∫–ª—é—á–∏—Ç–µ `quantized = true` (–µ—Å–ª–∏ –≤–∞—à–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–æ).
4. –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **vLLM**, –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –Ω–µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö.
   –í `settings.json`:
   ```json
   "roo-code.backend": "vllm",
   "roo-code.vllm_gpu_memory_utilization": 0.75,
   "roo-code.vllm_num_gpu": 1
   ```
5. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2‚Äë5‚ÄØ—Å–µ–∫—É–Ω–¥ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏).

---

## –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´—Å–±–æ–∏—Ç¬ª

| –ü—Ä–æ–±–ª–µ–º–∞ | –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å |
|----------|---------------|
| **CUDA out of memory** | –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ 4‚Äë–±–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `device_map="balanced_low_0"` –∏–ª–∏ offload to CPU. |
| **–ù–µ–Ω–∞–¥—ë–∂–Ω—ã–π –≤—ã–≤–æ–¥** | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `temperature=0` –∏ `do_sample=False`. |
| **–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ `max_length` –≤ `tokenizer` –º–µ–Ω—å—à–µ `num_ctx`. |
| **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à–æ–º** | –£–¥–∞–ª–∏—Ç–µ `~/.cache/huggingface/hub` –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ. |
| **–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö** | –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (‚âà200‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤). |


### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ VS‚ÄØCode —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏

1. **–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç** (–∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–æ –≤—ã–∑—ã–≤–∞–ª–æ —ç—Ç–æ—Ç –∫–æ–¥ —á–µ—Ä–µ–∑ CLI).
2. –í –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:
   - `model_name = gpt-oss/20b`
   - `num_ctx = 512`
   - `max_new_tokens = 256`
   - `temperature = 0.0`
3. –í–∫–ª—é—á–∏—Ç–µ `quantized = true` (–µ—Å–ª–∏ –≤–∞—à–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–æ).
4. –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **vLLM**, –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –Ω–µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö.
   –í `settings.json`:
   ```json
   "roo-code.backend": "vllm",
   "roo-code.vllm_gpu_memory_utilization": 0.75,
   "roo-code.vllm_num_gpu": 1
   ```
5. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2‚Äë5‚ÄØ—Å–µ–∫—É–Ω–¥ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏).

---

## –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´—Å–±–æ–∏—Ç¬ª

| –ü—Ä–æ–±–ª–µ–º–∞ | –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å |
|----------|---------------|
| **CUDA out of memory** | –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ 4‚Äë–±–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `device_map="balanced_low_0"` –∏–ª–∏ offload to CPU. |
| **–ù–µ–Ω–∞–¥—ë–∂–Ω—ã–π –≤—ã–≤–æ–¥** | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `temperature=0` –∏ `do_sample=False`. |
| **–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ `max_length` –≤ `tokenizer` –º–µ–Ω—å—à–µ `num_ctx`. |
| **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à–æ–º** | –£–¥–∞–ª–∏—Ç–µ `~/.cache/huggingface/hub` –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ. |
| **–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö** | –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (‚âà200‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤). |

   - `num_ctx = 512`
   - `max_new_tokens = 256`
   - `temperature = 0.0`
3. –í–∫–ª—é—á–∏—Ç–µ `quantized = true` (–µ—Å–ª–∏ –≤–∞—à–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–æ).
4. –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **vLLM**, –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ –Ω–µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö.
   –í `settings.json`:
   ```json
   "roo-code.backend": "vllm",
   "roo-code.vllm_gpu_memory_utilization": 0.75,
   "roo-code.vllm_num_gpu": 1
   ```
5. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2‚Äë5‚ÄØ—Å–µ–∫—É–Ω–¥ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏).

---

## –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´—Å–±–æ–∏—Ç¬ª

| –ü—Ä–æ–±–ª–µ–º–∞ | –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å |
|----------|---------------|
| **CUDA out of memory** | –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ 4‚Äë–±–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `device_map="balanced_low_0"` –∏–ª–∏ offload to CPU. |
| **–ù–µ–Ω–∞–¥—ë–∂–Ω—ã–π –≤—ã–≤–æ–¥** | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `temperature=0` –∏ `do_sample=False`. |
| **–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ `max_length` –≤ `tokenizer` –º–µ–Ω—å—à–µ `num_ctx`. |
| **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à–æ–º** | –£–¥–∞–ª–∏—Ç–µ `~/.cache/huggingface/hub` –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ. |
| **–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö** | –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (‚âà200‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤). |

   "roo-code.backend": "vllm",
   "roo-code.vllm_gpu_memory_utilization": 0.75,
   "roo-code.vllm_num_gpu": 1
   ```
5. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2‚Äë5‚ÄØ—Å–µ–∫—É–Ω–¥ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏).

---

## –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´—Å–±–æ–∏—Ç¬ª

| –ü—Ä–æ–±–ª–µ–º–∞ | –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å |
|----------|---------------|
| **CUDA out of memory** | –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ 4‚Äë–±–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `device_map="balanced_low_0"` –∏–ª–∏ offload to CPU. |
| **–ù–µ–Ω–∞–¥—ë–∂–Ω—ã–π –≤—ã–≤–æ–¥** | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `temperature=0` –∏ `do_sample=False`. |
| **–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ `max_length` –≤ `tokenizer` –º–µ–Ω—å—à–µ `num_ctx`. |
| **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à–æ–º** | –£–¥–∞–ª–∏—Ç–µ `~/.cache/huggingface/hub` –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ. |
| **–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö** | –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (‚âà200‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤). |

---
| **–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ `max_length` –≤ `tokenizer` –º–µ–Ω—å—à–µ `num_ctx`. |
| **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à–æ–º** | –£–¥–∞–ª–∏—Ç–µ `~/.cache/huggingface/hub` –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ. |
| **–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö** | –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∫—É—Å–æ–∫ –∫–æ–¥–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (‚âà200‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤). |

---
---

## –ë—ã—Å—Ç—Ä—ã–π —á–µ–∫‚Äë–ª–∏—Å—Ç –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

1. **–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç** (–ø—Ä–∏–º–µ—Ä: 200‚Äë300‚ÄØ—Ç–æ–∫–µ–Ω–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏).
2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å + –∫–≤–∞–Ω—Ç.** (gpt‚Äëoss:7b‚Äë4bit –∏–ª–∏ 13b‚Äë4bit, –µ—Å–ª–∏ 20b –Ω–µ –Ω—É–∂–µ–Ω).
3. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `num_ctx=512`** (–∏–ª–∏ `1024`, –µ—Å–ª–∏ GPU –ø–æ–∑–≤–æ–ª—è–µ—Ç).
4. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**: `max_new_tokens=256`, `temperature=0.0`, `top_p=0.95`.
5. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ —á–µ—Ä–µ–∑ vLLM** (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ).
   ```bash
   vllm serve gpt-oss/20b --tensor-parallel-size 1 --max-model-len 512 --dtype bfloat16
   ```
6. **–û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–ø—Ä–æ—Å** —á–µ—Ä–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –∂–¥–∏—Ç–µ 2‚Äë5‚ÄØ—Å–µ–∫—É–Ω–¥.

–ï—Å–ª–∏ –ø–æ—Å–ª–µ —ç—Ç–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –≤—Å—ë –µ—â—ë –≤–æ–∑–Ω–∏–∫–∞—é—Ç —Å–±–æ–∏, –¥–∞–π—Ç–µ –∑–Ω–∞—Ç—å, –∫–∞–∫–æ–π –∏–º–µ–Ω–Ω–æ —Å—Ç–µ–∫ (Python‚Äë–≤–µ—Ä—Å–∏—è, CUDA, –≤–µ—Ä—Å–∏—è `transformers`, —Ä–∞–∑–º–µ—Ä GPU) –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, –∏ —è –ø–æ–¥–±–µ—Ä—É –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ
—Ä–µ—à–µ–Ω–∏—è. –£–¥–∞—á–∏ —Å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–æ–º! üöÄ

>>> –£ Roo –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã...
... –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Å–±–æ–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, —á—Ç–æ –º–æ–∂–Ω–æ —Å–º—è–≥—á–∏—Ç—å —Å –ø–æ–º–æ—â—å—é —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–±–∏—Ç—å –∑–∞–¥–∞—á—É –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —à–∞–≥–∏").
Thinking...
We need to respond in Russian, acknowledging issues, suggest steps, maybe mention debugging. Provide user guidance: break task into smaller steps, reduce context, use incremental generation, etc. Also mention potential solutions:       
reloading model, checking memory, adjust num_ctx, use smaller model. Provide step-by-step.
...done thinking.

### –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç

–ö–æ–≥–¥–∞ –≤—ã –≤–∏–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤—Ä–æ–¥–µ:

> **¬´–£ Roo –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã‚Ä¶ –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Å–±–æ–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏¬ª**

‚Äî —ç—Ç–æ –æ–±—ã—á–Ω–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –ª–∏–±–æ ¬´–∑–∞–ª–∏–ø–ª–∞¬ª –≤ —Å–≤–æ—ë–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º ¬´–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏¬ª –∏ –Ω–µ –º–æ–∂–µ—Ç –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ª–∏–±–æ –æ–Ω–∞ –∏—Å—á–µ—Ä–ø–∞–ª–∞ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–π –µ–π —Ä–µ—Å—É—Ä—Å (–ø–∞–º—è—Ç—å, —Ç–∞–π–º–∞—É—Ç).  

–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ, –∫–∞–∫ –±—ã—Å—Ç—Ä–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —ç—Ç–∏ —Å–∏—Ç—É–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ ¬´–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ¬ª —Å–æ–≤–µ—Ç—ã –ø–æ —Ä–∞–±–æ—Ç–µ —Å LLM –≤ –≤–∞—à–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏.

---

## 1. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏

| –®–∞–≥ | –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å | –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å |
|-----|---------------|---------------|
| **1.1** | **–ü–∞–º—è—Ç—å GPU** | –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `nvidia-smi`. –ï—Å–ª–∏ GPU –ø–æ—á—Ç–∏ 100‚ÄØ% –ø–∞–º—è—Ç–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:<br>‚Ä¢ –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ 4‚Äë–±–∏—Ç/8‚Äë–±–∏—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é.<br>‚Ä¢ –£–º–µ–Ω—å—à–∏—Ç—å `num_ctx` (–¥–æ 512). |
| **1.2** | **–ö—ç—à –º–æ–¥–µ–ª–∏** | –ò–Ω–æ–≥–¥–∞ –∫—ç—à –ø–æ–≤—Ä–µ–∂–¥–∞–µ—Ç—Å—è. –û—á–∏—Å—Ç–∏—Ç–µ –µ–≥–æ: `rm -rf ~/.cache/huggingface/hub`. –ó–∞—Ç–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode. |
| **1.3** | **–í–µ—Ä—Å–∏—è `transformers`** | –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å >=‚ÄØ4.34 (–∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–æ—Å—Ç—É–ø–Ω–∞—è). –°—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –º–æ–≥—É—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É–ø—Ä–∞–≤–ª—è—Ç—å –ø–∞–º—è—Ç—å—é. |
| **1.4** | **–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞** | –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–Ω–∏–º–∞–µ—Ç >‚ÄØ60‚ÄØ—Å, –∑–∞–¥–∞–π—Ç–µ —Ç–∞–π–º–∞—É—Ç –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ (`request_timeout=30`). |
| **1.5** | **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã** | –ï—Å–ª–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤, –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ `max_batch_size` –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç–µ –æ—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤. |

---

## 2. –ö–∞–∫ ¬´–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏¬ª —Ä–∞–∑–±–∏—Ç—å –∑–∞–¥–∞—á—É

1. **–†–∞–∑–¥–µ–ª–∏—Ç–µ –Ω–∞ ¬´—à–∞–≥–∏¬ª**  
   - **–®–∞–≥ 1**: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–¥–∞ (–ø–æ–ª—É—á–µ–Ω–∏–µ –Ω—É–∂–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞).  
   - **–®–∞–≥ 2**: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤/–ø—Ä–∏–º–µ—á–∞–Ω–∏–π.  
   - **–®–∞–≥ 3**: –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.  
   - **–®–∞–≥ 4**: –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞.

2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ ¬´—Å—Ä–µ–∑—ã¬ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**  
   –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Ü–µ–ª—ã–π —Ñ–∞–π–ª, –ø–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ 200‚Äë300 —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. –≠—Ç–æ —É–º–µ–Ω—å—à–∞–µ—Ç `num_ctx` –∏ —É—Å–∫–æ—Ä—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.

3. **–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ ¬´–≤—ã–≤–æ–¥—ã¬ª**  
   –ó–∞–ø—É—Å–∫–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ `do_sample=False` —Å `max_new_tokens=128`, –ø–æ–ª—É—á–∞–π—Ç–µ —á–∞—Å—Ç–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç, –∑–∞—Ç–µ–º —É—Ç–æ—á–Ω—è–π—Ç–µ (–ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å ¬´–ø—Ä–æ–±–Ω–æ–≥–æ¬ª –ø—Ä–æ–º–ø—Ç–∞). –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç ¬´–∑–∞–≤–∏—Å–∞–Ω–∏–µ¬ª –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–∫–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

---

## 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—á–µ–º—É |
|----------|------------------------|--------|
| `model_name` | `gpt-oss/20b` | –ï—Å–ª–∏ –≤–∞—à GPU –ø–æ–∑–≤–æ–ª—è–µ—Ç. |
| `quantization` | `load_in_4bit=True` | –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ. |
| `device_map` | `"auto"` | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ–∂–¥—É GPU/CPU. |
| `num_ctx` | `512` | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –Ω–µ —Ç—Ä–∞—Ç–∏—Ç –ø–∞–º—è—Ç—å. |
| `max_new_tokens` | `256` | –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π. |
| `temperature` | `0.0` | –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. |
| `top_p` | `0.95` | –ù–µ–º–Ω–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏, –Ω–æ –±–µ–∑ ¬´—à—É–º–æ–≤¬ª. |
| `batch_size` | `1` | –ï—Å–ª–∏ –≤—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è. |

---

## 4. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ ¬´—á–µ–ª–ª–µ–Ω–¥–∂–∏¬ª –≤ Roo

–ï—Å–ª–∏ Roo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä, –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å ¬´–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π¬ª —à–∞–≥:

```python
# –í —Ñ–∞–π–ª–µ config.yml
step_strategy: "split_and_refine"   # –†–∞–∑–±–∏–≤–∞—Ç—å –∑–∞–¥–∞—á—É –Ω–∞ —à–∞–≥–∏
max_retry: 3                        # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ç—å –ø—Ä–∏ —Å–±–æ–µ
retry_delay: 2                      # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º
```

–¢–æ–≥–¥–∞ –ø—Ä–∏ —Å–±–æ–µ –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–µ—Ä–Ω—ë—Ç—Å—è –∫ —à–∞–≥—É ¬´–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–¥–∞¬ª, –∞ –Ω–µ –∫ ¬´–ø–æ–ª–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏¬ª.

---

## 5. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä ¬´—Ä–∞–∑–±–∏–≤–∫–∏¬ª –∑–∞–¥–∞—á–∏

```python
# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è
def foo(x):
    y = x * 2
    return y

# 1. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
comment_prompt = "Add a docstring to the following function:\n\n" + get_function_source(foo)
comment = generate(comment_prompt, max_new_tokens=64)

# 2. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 200 —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
refactor_prompt = f"""Refactor this function to be more Pythonic:
{comment}
{get_function_source(foo)}"""
refactor = generate(refactor_prompt, max_new_tokens=128)

print(refactor)
```

–ï—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ¬´–ø–∞–¥–∞–µ—Ç¬ª –Ω–∞ —ç—Ç–∞–ø–µ `comment`, –º—ã –ø–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —à–∞–≥, –∞ –Ω–µ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å.

---

## 6. –ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ —Å–±–æ–π –≤—Å—ë‚Äë—Ç–∞–∫–∏ –æ—Å—Ç–∞—ë—Ç—Å—è?

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏** (`extension.log` –≤ VS‚ÄØCode) ‚Äì –∏—â–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–∏–ø–∞ `CUDA out of memory`, `tokenizer error` –∏–ª–∏ `RuntimeError`.  
2. **–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å**:  
comment_prompt = "Add a docstring to the following function:\n\n" + get_function_source(foo)
comment = generate(comment_prompt, max_new_tokens=64)

# 2. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 200 —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
refactor_prompt = f"""Refactor this function to be more Pythonic:
{comment}
{get_function_source(foo)}"""
refactor = generate(refactor_prompt, max_new_tokens=128)

print(refactor)
```

–ï—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ¬´–ø–∞–¥–∞–µ—Ç¬ª –Ω–∞ —ç—Ç–∞–ø–µ `comment`, –º—ã –ø–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —à–∞–≥, –∞ –Ω–µ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å.

---

## 6. –ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ —Å–±–æ–π –≤—Å—ë‚Äë—Ç–∞–∫–∏ –æ—Å—Ç–∞—ë—Ç—Å—è?

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏** (`extension.log` –≤ VS‚ÄØCode) ‚Äì –∏—â–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–∏–ø–∞ `CUDA out of memory`, `tokenizer error` –∏–ª–∏ `RuntimeError`.
2. **–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å**:
{get_function_source(foo)}"""
refactor = generate(refactor_prompt, max_new_tokens=128)

print(refactor)
```

–ï—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ¬´–ø–∞–¥–∞–µ—Ç¬ª –Ω–∞ —ç—Ç–∞–ø–µ `comment`, –º—ã –ø–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —à–∞–≥, –∞ –Ω–µ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å.

---

## 6. –ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ —Å–±–æ–π –≤—Å—ë‚Äë—Ç–∞–∫–∏ –æ—Å—Ç–∞—ë—Ç—Å—è?

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏** (`extension.log` –≤ VS‚ÄØCode) ‚Äì –∏—â–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–∏–ø–∞ `CUDA out of memory`, `tokenizer error` –∏–ª–∏ `RuntimeError`.
2. **–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å**:
   ```bash
   # –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ vLLM
   pkill -f "vllm serve"
   vllm serve gpt-oss/20b --max-model-len 512 --tensor-parallel-size 1 --dtype bfloat16
   ```
1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏** (`extension.log` –≤ VS‚ÄØCode) ‚Äì –∏—â–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–∏–ø–∞ `CUDA out of memory`, `tokenizer error` –∏–ª–∏ `RuntimeError`.
2. **–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å**:
   ```bash
   # –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ vLLM
   pkill -f "vllm serve"
   vllm serve gpt-oss/20b --max-model-len 512 --tensor-parallel-size 1 --dtype bfloat16
   ```
   ```bash
   # –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ vLLM
   pkill -f "vllm serve"
   vllm serve gpt-oss/20b --max-model-len 512 --tensor-parallel-size 1 --dtype bfloat16
   ```
   pkill -f "vllm serve"
   vllm serve gpt-oss/20b --max-model-len 512 --tensor-parallel-size 1 --dtype bfloat16
   ```
   vllm serve gpt-oss/20b --max-model-len 512 --tensor-parallel-size 1 --dtype bfloat16
   ```
   ```
3. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ—Ç—å** ‚Äì –∏–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤, –∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å–±–æ—é.
4. **–û–±–Ω–æ–≤–∏—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã** GPU (CUDA 12.x vs 11.x) ‚Äì –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –ø–∞–º—è—Ç–∏.
5. **–°–≤—è–∂–∏—Ç–µ—Å—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π** (–µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ `gpt-oss` —á–µ—Ä–µ–∑ API), —Å–æ–æ–±—â–∏—Ç–µ `request_id` –∏ –ª–æ–≥–∏.

---

## 7. –ò—Ç–æ–≥–æ–≤—ã–π —á–µ–∫‚Äë–ª–∏—Å—Ç

- [ ] –£–º–µ–Ω—å—à–∏—Ç—å `num_ctx` –¥–æ 512.  
- [ ] –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ 4‚Äë–±–∏—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é.  
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `do_sample=False`, `temperature=0`.  
- [ ] –†–∞–∑–±–∏—Ç—å –∑–∞–¥–∞—á—É –Ω–∞ —à–∞–≥–∏ (–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ‚Üí —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥).  
- [ ] –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ.
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å GPU.
- [ ] –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´–∑–∞–ª–∏–ø–∞–µ—Ç¬ª, –∑–∞–¥–∞–π—Ç–µ `max_retry` –∏ `retry_delay`.

–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç–∏ —à–∞–≥–∏ –∏ –¥–∞–π—Ç–µ –∑–Ω–∞—Ç—å, –∫–∞–∫–æ–π –∏–∑ –Ω–∏—Ö –ø–æ–º–æ–≥, –∞ –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –æ—Å—Ç–∞–ª–∞—Å—å ‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ —á–∞—Å—Ç—å –ª–æ–≥–∞ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤/–ø–∞—Ä–æ–ª–µ–π), –∏ —è –ø–æ–º–æ–≥—É –¥–∞–ª—å—à–µ. üöÄ

>>> –∞ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å codellama:7b –∫–∞–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω—É–∂–Ω—ã –≤ VScode 
‚†º Thinking...
We need to answer: If using CodeLlama 7B model, what settings are needed in VS Code? Provide config settings: num_ctx, token limits, quantization, model loading options, maybe use llama.cpp, vLLM or flash-attn, CPU/GPU usage. 
Provide suggestions for speed, memory, context. Provide VSCode extension settings and maybe environment variables. Provide how to load CodeLlama:7B. Provide example launch. Provide steps for quantization, GPU. Provide explanation       
of how to configure the VS Code extension "roo-code" or generic. Provide typical settings: max_new_tokens, temperature, top_p. Also provide recommended batch size. Provide how to use CodeLlama on CPU vs GPU. Also include how to         
set up environment with Llama.cpp or vLLM. Provide mention of huggingface Hub. Provide example .json config. Provide example python snippet to load CodeLlama 7B using transformers and bitsandbytes. Provide suggestion to reduce
context to 2048 tokens. Provide use of Llama-7b-quantized-4bit. Provide mention of memory: 7B requires ~7GB VRAM (or 7*2=14GB float32). Provide 4-bit quantization reduces to ~1GB. Provide set device_map to auto. Provide steps to        
install bitsandbytes. Provide mention of using LoRA or quantization. Provide details for VS Code: set "modelName": "codellama/CodeLlama-7b-hf", "num_ctx": 2048, "quantization": "4bit". Provide "max_new_tokens": 256. Provide 
"temperature": 0.1, "top_p": 0.95. Provide "maxRequestSize": maybe 8? Provide suggestion for caching. Provide mention of using "useVLLM": true. Provide example extension settings. Also mention about setting "runtime": 
"transformers" vs "vllm". Provide suggestions. Provide final answer.
...done thinking.

## –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å VS‚ÄØCode (–∏–ª–∏ –ª—é–±–æ–π LLM‚Äë–ø–ª–∞–≥–∏–Ω) –¥–ª—è **CodeLlama‚Äë7B**

–ù–∏–∂–µ ‚Äì ¬´—Ä–∞–±–æ—á–∏–π —á–µ–∫‚Äë–ª–∏—Å—Ç¬ª‚ÄØ‚Äì —á—Ç–æ –ø—Ä–æ–ø–∏—Å–∞—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ, –∫–∞–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∫–æ–º–∞–Ω–¥—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å, –∏ –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—ã—á–Ω–æ –¥–∞—é—Ç –ª—É—á—à—É—é —Å–∫–æ—Ä–æ—Å—Ç—å/—ç–∫–æ–Ω–æ–º–∏—é –ø–∞–º—è—Ç–∏.

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ß—Ç–æ –∑–∞–¥–∞—ë–º | –ü–æ—á–µ–º—É |
|----------|------------|--------|
| **modelName** | `codellama/CodeLlama-7b-hf` | –°–∞–º—ã–π –ø—Ä—è–º–æ–π HF‚Äë–º–æ–¥–µ–ª—å‚Äë–∫–ª—é—á. |
| **quantization** | `load_in_4bit=True` (–∏–ª–∏ `4bit`) | –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å (‚âà‚ÄØ1‚ÄØGB –≤–º–µ—Å—Ç–æ 7‚ÄØGB VRAM). |
| **device_map** | `"auto"` | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ—Ä—ë—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ GPU‚Äë—è–¥—Ä–∞ –∏/–∏–ª–∏ CPU. |
| **num_ctx** | `2048` (–∏–ª–∏ –º–µ–Ω—å—à–µ,‚ÄØ1500‚Äë1800 ‚Äì –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞) | –û—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞, –Ω–æ –Ω–µ ¬´–ø–µ—Ä–µ—Ö–æ–¥–∏—Ç¬ª –≤ –ø–∞–º—è—Ç—å. |
| **max_new_tokens** | `256`‚Äì`512` | –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏: 256‚Äë–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π, 512‚Äë–¥–ª—è –±–æ–ª—å—à–∏—Ö. |
| **temperature** | `0.0`‚Äì`0.2` | –ü—Ä–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∏–∑–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å ¬´–∑–∞—à–∏–ø–∞–ª–∞¬ª –∫ –æ–¥–Ω–æ–º—É –≤—ã—Ö–æ–¥—É. |
| **top_p** | `0.95`‚Äì`1.0` | –ú–∞–ª–µ–Ω—å–∫–∞—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å, –Ω–æ –±–µ–∑ ¬´—à—É–º–æ–≤¬ª. |
| **max_batch_size** | `1` (–∏–ª–∏ `2`/`4` –ø—Ä–∏ vLLM) | –ï—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω—è—Ç—å –ø–∞–º—è—Ç—å. |
| **cache_dir** | `~/.cache/huggingface/llama` | –£–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –≤–µ—Å–∞–º. |
| **runtime** | `transformers`‚ÄØ/‚ÄØ`vllm`‚ÄØ/‚ÄØ`llama.cpp` | –í—ã–±–∏—Ä–∞–π—Ç–µ, –∫–∞–∫ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª–Ω—è—Ç—å—Å—è –º–æ–¥–µ–ª—å. –ù–∏–∂–µ –ø—Ä–∏–º–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–π –æ–ø—Ü–∏–∏. |
| **timeout** | `30`‚ÄØ—Å–µ–∫ | –ß—Ç–æ–±—ã –Ω–µ ¬´–∑–∞–ª–∏–ø–∞—Ç—å¬ª –Ω–∞ –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö. |
| **retry_on_fail** | `true` | –ê–≤—Ç–æ–ø–æ–¥–Ω—è—Ç–∏–µ —à–∞–≥–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–¥–∞ ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Üí —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥). |

---

## 1. 1‚Äë—à–∞–≥: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —Å–æ–±—Ä–∞—Ç—å –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å transformers (‚â•‚ÄØ4.34)
pip install --upgrade transformers
# 2. bitsandbytes –¥–ª—è 4‚Äë–±–∏—Ç
pip install bitsandbytes
# 3. –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ vLLM, —Å–∫–∞—á–∞–π—Ç–µ –µ–≥–æ
pip install vllm
# 4. –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ llama.cpp, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫—É (—Å–º. —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π)
# git clone https://github.com/ggerganov/llama.cpp
# cd llama.cpp && make
```

---

## 2. –ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å CodeLlama‚Äë7B –≤ Python (–¥–ª—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import bitsandbytes as bnb

model_name = "codellama/CodeLlama-7b-hf"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,          # float16 (‚âà‚ÄØ7‚ÄØGB VRAM)
    device_map="auto",                  # –∞–≤—Ç–æ‚Äë—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    load_in_4bit=True,                  # 4‚Äë–±–∏—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (‚âà‚ÄØ1‚ÄØGB VRAM)
    quantization_config=bnb.nn.Linear8BitLt,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
```

*–ï—Å–ª–∏ –ø–∞–º—è—Ç–∏ —Ö–≤–∞—Ç–∞–µ—Ç* (`~7‚ÄØGB VRAM`) –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å `torch_dtype=torch.float16`.  
*–î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏* ‚Äì –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `load_in_4bit=True` (–∏, –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏, `bnb.nn.Linear8BitLt`).

---

## 3. –ö–∞–∫ –≤–∫–ª—é—á–∏—Ç—å **vLLM** –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏

```bash
# –ó–∞–ø—É—Å–∫–∞–µ–º vLLM –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
vllm serve codellama/CodeLlama-7b-hf \
    --max-model-len 2048 \
    --tensor-parallel-size 1 \
    --dtype bfloat16     # –∏–ª–∏ float16/float32
```

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ API‚Äë—ç–Ω–¥–ø–æ–∏–Ω—Ç –æ–±—ã—á–Ω–æ –±—É–¥–µ—Ç `http://127.0.0.1:8000/v1/completions`.

–í VS‚ÄØCode (—á–µ—Ä–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä `roo-code`) —É–∫–∞–∂–∏—Ç–µ:

```json
{
  "modelName": "codellama/CodeLlama-7b-hf",
  "runtime": "vllm",
  "vllmHost": "http://127.0.0.1:8000",
  "num_ctx": 2048,
  "max_new_tokens": 256,
  "temperature": 0.0,
  "top_p": 0.95,
  "batchSize": 1,
  "requestTimeout": 30
}
```

---

## 4. –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å **—Ç–æ–ª—å–∫–æ –Ω–∞ CPU**

```bash
pip install transformers==4.34
pip install sentencepiece
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "codellama/CodeLlama-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,   # CPU –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å float32/16
    device_map="cpu"             # –≤—Å—ë –Ω–∞ CPU
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
```

CPU‚Äë–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±—É–¥–µ—Ç ~‚ÄØ30‚Äë50√ó –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –º–æ–∂–µ—Ç —Ö–≤–∞—Ç–∏—Ç—å –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –µ—Å–ª–∏ —É –≤–∞—Å –Ω–µ —Ç–∞–∫ –º–Ω–æ–≥–æ –∫–æ–¥–∞.

---

## 5. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å CodeLlama‚Äë7B –≤ VS‚ÄØCode

| –ù–∞—Å—Ç—Ä–æ–π–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—è—Å–Ω–µ–Ω–∏–µ |
|-----------|----------|-----------|
| `modelName` | `"codellama/CodeLlama-7b-hf"` | –ó–∞–¥–∞—ë—Ç —Ç–æ—á–Ω—ã–π HF‚Äë–º–æ–¥–µ–ª—å. |
| `quantization` | `"4bit"` | –ü—Ä–∏–Ω–∏–º–∞–µ—Ç 4‚Äë–±–∏—Ç, —ç–∫–æ–Ω–æ–º–∏—Ç ~‚ÄØ10√ó –ø–∞–º—è—Ç—å. |
| `num_ctx` | `2048` | Standard context –¥–ª—è LLaMA‚Äë7B. |
| `max_new_tokens` | `256` | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. |
| `temperature` | `0.0` | –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ. |
| `top_p` | `0.95` | –í–∫–ª—é—á–∞–µ–º —á—É—Ç—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. |
| `runtime` | `"vllm"` | –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç flash‚Äëattn. |
| `batchSize` | `1` | –ß—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å. |
| `requestTimeout` | `30` | –î–ª—è –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. |
| `retryOnFail` | `true` | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —à–∞–≥—É –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏. |

---

## 6. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (`.json`)

```json
{
  "modelName": "codellama/CodeLlama-7b-hf",
  "runtime": "vllm",
  "vllmHost": "http://127.0.0.1:8000",
  "num_ctx": 2048,
  "max_new_tokens": 256,
  "temperature": 0.0,
  "top_p": 0.95,
  "batchSize": 1,
  "requestTimeout": 30,
  "retryOnFail": true,
  "retryDelay": 2
}
```

> **Tip**: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å ¬´–∑–∞–ª–∏–ø–∞–µ—Ç¬ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç `null`, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á `stream: true` –≤ –∑–∞–ø—Ä–æ—Å ‚Äì —ç—Ç–æ –∑–∞—Å—Ç–∞–≤–∏—Ç LLaMA –æ—Ç–¥–∞–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å—Ç—Ä–æ—á–Ω–æ –∏ –¥–∞—Å—Ç –≤–∞–º —à–∞–Ω—Å ¬´–ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å¬ª –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.

---

## 7. –ß—Ç–æ –¥–∞–ª—å—à–µ?

1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Å—ë –≤—ã—à–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω–æ–µ.  
2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ VS‚ÄØCode –∏ –≤–∞—à –ø–ª–∞–≥–∏–Ω.  
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ `codellama/CodeLlama-7b-hf` —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è (–æ—à–∏–±–æ–∫ ¬´–≤–µ—Å–æ–≤¬ª/¬´–≥–¥–µ¬ª).  
4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –Ω–µ–±–æ–ª—å—à–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å.  

–ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å—ë –µ—â—ë ¬´–∑–∞–ª–∏–ø–∞–µ—Ç¬ª, –ø—Ä–∏—à–ª–∏—Ç–µ –ª–æ–≥‚Äë–≤—ã–≤–æ–¥ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤), –∏ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º, –Ω—É–∂–Ω–æ –ª–∏ –º–µ–Ω—è—Ç—å `num_ctx` –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç—å 8‚Äë–±–∏—Ç/16‚Äë–±–∏—Ç.